# Bad Data: Recognition and Recovery

Reference file for Jadlis-Advisor-MomTest. Load this when a user shares conversation transcripts, quotes, or feedback and needs help evaluating signal quality — or when diagnosing why their discovery conversations feel positive but yield no useful learning.

---

## The Core Problem

Customer conversations generate two kinds of output: signal and noise. Most founders mistake noise for signal. The reason: noise arrives in forms that feel good — agreement, enthusiasm, encouragement. Signal often arrives as friction, silence, or inconvenient specificity.

Three categories of bad data account for the vast majority of what people bring back from "successful" customer meetings: compliments, fluff, and feature requests. Learn to identify all three on contact. Each has a specific recovery move.

---

## Category 1: Compliments

### What They Look Like

- "That's a great idea."
- "I love this, keep me in the loop."
- "Wow, this is really cool."
- "I'd totally use this."
- "This is exactly what we need."
- "You're onto something."
- "Send me a link when it launches."

### Why They're Worthless

Compliments cost the speaker nothing. They are social lubricant, not data. A person can genuinely like your idea and still never use your product, never pay for it, never change their behavior for it. Liking requires no commitment. Purchasing, switching, and adopting do.

The danger of compliments is proportional to how good they feel. A meeting full of compliments produces a founder who reports "really positive feedback" and "everyone loves the concept." This is the worst outcome — you leave with false confidence and zero usable information.

Diagnostic phrases that signal compliment contamination in a debrief:
- "The meeting went really well."
- "They were super enthusiastic."
- "Everyone we talk to loves the idea."
- "We're getting really positive signals."

When you hear these, the meeting probably yielded nothing actionable.

### Recovery: Deflect and Redirect

Do not engage with the compliment. Do not say "thanks." Deflect immediately to facts.

- "That's great to hear — how are you currently dealing with this problem?"
- "I'm glad it resonates — walk me through what you're doing today when this comes up."
- "Good to know. What did you try before landing on your current approach?"

The goal: convert the complimentary energy into a concrete account of past behavior. The compliment tells you nothing. What they did last Tuesday does.

Rule: **Compliments are the fool's gold of customer learning.** Shiny, abundant, and worthless.

---

## Category 2: Fluff

Fluff is any response that fails to anchor to specific, past, real behavior. It comes in three sub-types.

### Sub-type 2a: Generic Claims

Statements about habitual behavior that don't reference a specific instance:
- "I usually do it this way..."
- "We always handle that with..."
- "I never bother with..."
- "Typically our team just..."

Generic claims describe how people think they behave, not how they actually behave. Research on self-reporting consistently shows humans are poor observers of their own habits. What they report as "always" turns out to be "sometimes" or "when I remember" when examined against actual instances.

Treat generic claims as hypotheses to be verified, not as facts.

### Sub-type 2b: Future Promises

The most dangerous category of bad data:
- "I would definitely buy that."
- "I will switch as soon as you launch."
- "I'd pay X for that."
- "We would definitely use this."
- "Once you have Y feature, we're in."

**Anything involving the future is an over-optimistic lie.** Not malicious — people believe what they say in the moment. But stated future intention does not predict behavior. The gap between "I would buy" and "I did buy" is where most startups discover their real market is far smaller than their survey responses suggested.

"I would definitely buy that" is probably the single most dangerous sentence in customer discovery. It sounds like validation. It is a polite way of ending the conversation without committing to anything.

### Sub-type 2c: Hypothetical Maybes

Softer futures, equally unreliable:
- "I might look into that."
- "I could see us using something like this."
- "That might work for our situation."
- "I'd have to think about it."

These feel like 60% yeses. They are closer to 0%.

### Fluff-Inducing Questions

Certain question structures reliably produce fluff. Identify them in your question set and remove them:

| Fluff-inducing | Why it fails |
|---|---|
| "Do you ever...?" | Invites yes/no + generic claim |
| "Would you ever...?" | Invites future promise |
| "What do you usually do when...?" | Invites generic claim |
| "Could you see yourself using...?" | Invites hypothetical maybe |
| "How often do you...?" | Invites frequency estimate, not instance |
| "Would that be useful?" | Invites compliment |

Rule: **Opinions are worthless. Anything involving the future is an over-optimistic lie.**

### Recovery: Anchor to Specifics

Convert every generic or future statement to a specific past instance.

- Generic claim → "When's the last time that happened?"
- Generic claim → "Can you walk me through the last time you did it that way?"
- Future promise → "Have you looked for a solution to this before?"
- Future promise → "What have you tried so far?"
- Hypothetical maybe → "Is this something you've actually tried to fix?"
- Hypothetical maybe → "How much time/money did you spend on it last quarter?"

The anchor question is: **"When's the last time that happened?"** Then: **"Can you walk me through that?"**

If they cannot produce a specific instance, either the problem doesn't occur as frequently as they claim, or it doesn't bother them enough to notice.

---

## Category 3: Feature Requests and Ideas

### What They Look Like

- "It would be great if it also did X."
- "Have you thought about adding Y?"
- "What would really make this work is Z."
- "Can you integrate with [tool]?"
- "I'd use it if it had [feature]."

### Why They're Traps

Feature requests feel like useful input. They are partially useful — they reveal that the person is engaged enough to imagine the product working. But the feature they request is almost never the feature that would actually solve their problem. It is the feature their current mental model suggests would help.

Do not implement features from customer requests. Dig for the underlying motivation instead.

The request is a symptom. The disease is a workflow, a frustration, or an unmet goal. You need the disease, not the symptom.

### Recovery: Dig for Motivation

- "Why do you want that?"
- "What would that let you do that you can't do now?"
- "How are you coping without it?"
- "What would you do if we never built that?"
- "Is this blocking you from using it, or is it a nice-to-have?"
- "Should we delay launch to add that, or could it come later?"

Write every feature request down. Do not promise to build it. Do not dismiss it. Ask what motivated it. The motivation is the signal; the feature is just a proposed solution.

Rule: **Note feature requests but never obey them blindly. Dig until you find the underlying goal.**

---

## Approval-Seeking Patterns

Bad data often originates on the founder's side, not the customer's. Watch for these patterns in how questions get asked.

### Fishing for Compliments

Questions designed (consciously or not) to generate validation:
- "Do you think this will work?"
- "Do you like what we're building?"
- "Honestly, does this seem like a good idea?"
- "What do you think of this concept?"

These questions have a correct answer that the customer can see. They will give it to you. This is not their fault.

### The Pathos Problem

Exposing your emotional investment before asking for feedback ruins the feedback. If you say "This is the idea I quit my job for" or "I've been working on this for two years," you have made the customer responsible for your wellbeing. They will lie to protect you. Even "Be honest with me" doesn't override this — you've already told them what's at stake.

Rule: **If you've mentioned your idea, people will try to protect your feelings.** This is human. It is also useless to you.

Keep your emotional investment out of the conversation until after you have the data.

### Pitch Mode

Recognizable by:
- "No, no — I think you might be misunderstanding what we're building."
- "But it also does this — did I mention the part where..."
- Holding the conversation hostage until the customer says something positive
- Defending the idea against every expressed doubt

Pitch mode ends the discovery conversation. You cannot learn and pitch simultaneously. When you pitch, the customer's job becomes politely surviving your enthusiasm. Their feedback becomes performance, not information.

Recovery script when you catch yourself in pitch mode: "Whoops — I just slipped into pitch mode. Sorry about that. Can we go back to what you were saying about [their last point]?"

---

## Good vs. Bad Questions

### The Rewrite Table

| Bad question | Why it fails | Better version |
|---|---|---|
| "Do you think it's a good idea?" | Invites compliment | "How do you currently handle [problem]? What's that costing you?" |
| "Would you buy a product that did X?" | Future promise, hypothetical | "How do you currently solve X? What have you tried? How much do you spend on it?" |
| "How much would you pay for X?" | Hypothetical; anchors to arbitrary number | "How much does this problem cost you now? What's your current spend on it?" |
| "What would your dream product do?" | Generates feature wish list | Ask this, but immediately follow: "Why do you want that? What would that let you do?" |
| "Would you pay $Y for Z?" | Leads to hypothetical future commitment | "What's your budget for solving this? What are you spending now?" |
| "Do you like it?" | Fishing for compliment | Don't ask. Watch what they do with it. |
| "Would you use something like this?" | Future promise + hypothetical | "Have you ever paid for something to solve this? What was it?" |

### Questions That Produce Real Signal

Memorize these. Deploy them by default.

**"Why do you bother?"**
Cuts from the surface problem to the real motivation. Use it when someone describes a workflow — ask why they do it that all.

**"What are the implications of that?"**
Distinguishes problems that actually matter from problems people mention because they came to mind. A real problem has real downstream consequences. A trivial problem produces vague answers to this question.

**"Talk me through the last time that happened."**
The single most reliable question in customer discovery. Forces specificity. Cannot be answered with generics or futures. Produces observable behavior data.

**"What else have you tried?"**
Reveals whether they care enough to have acted. If they haven't tried to solve it, they are unlikely to try your solution. Rule: **If they haven't looked for ways of solving it already, they're not going to look for yours.**

**"How are you dealing with it now?"**
Two outputs: (1) reveals the current solution you are competing against, (2) reveals what they are actually paying/spending (time or money), which is your real price anchor.

**"Where does the money come from?"**
Essential in B2B. Reveals the actual buyer, budget owner, and procurement process. Never assume the person you're talking to controls the budget.

**"Who else should I talk to?"**
If the conversation was valuable, they will refer you. If they won't, that is also data. Use this to generate pipeline.

**"Is there anything else I should have asked?"**
Gives the customer permission to correct your framing. They often know what you're missing better than you do. Use at the end of every conversation.

---

## Emotional Signals

Emotional signals are not bad data — but they require excavation before they become useful.

Emotional signals look like:
- Visible frustration when describing a workflow
- "That drives me absolutely crazy"
- Sighing, laughing bitterly, rolling their eyes
- "Don't even get me started on..."
- Excitement or energy that spikes on a specific topic

Emotional signals tell you: this matters. They do not tell you why it matters, what causes it, or whether you can solve it.

Recovery: dig immediately.
- "Tell me more about that."
- "That seems to really bother you."
- "What makes it so frustrating?"
- "What happens when it goes wrong?"
- "Why haven't you fixed it?"

The emotional signal points to the location. The follow-up excavates the shape and depth of the problem.

---

## Signal Strength Hierarchy

Rank signals from most to least reliable. Weight them accordingly.

**Tier 1 — Hardest evidence**
Fact anchored to a specific past event, with concrete details (date, amount, sequence of actions). "Last Thursday I spent four hours manually reformatting the export from [tool] before I could send the report." Treat as close to ground truth as discovery allows.

**Tier 2 — Demonstrated action**
They actually did something: built a workaround, hired someone, paid for a tool, changed their process. Behavior beats stated preference every time.

**Tier 3 — Commitment with cost**
They have pre-ordered, given up time for a pilot, introduced you to their boss, or put their reputation behind something. Any commitment that costs them something is meaningful.

**Tier 4 — Emotional signal**
Real, but requires excavation. Points to territory; doesn't map it.

**Tier 5 — Stated preference**
"I prefer to do it this way." Some signal. Verify against actual behavior whenever possible.

**Tier 6 — Future promise**
"I would buy that." Nearly worthless. Count it as a weak hypothesis, not as validation.

**Tier 7 — Compliment**
Zero signal value. Discard.

### Calibration Examples from the Field

**The loyalty card complainer.** A person complains loudly and specifically about loyalty card chaos — too many physical cards, no organization. Sounds like a strong signal for a loyalty card app. But: when asked whether they have looked for an existing app, the answer is no. When asked how often they actually use loyalty cards, the frequency drops. The emotional energy was real; the problem severity was not. They have not acted. Treat as Tier 5 at best.

**The inbox zero zealot.** Someone passionate about email productivity, describes their elaborate system in detail, gets animated talking about the cost of a disorganized inbox. Then mentions: their inbox fell apart for ten days while they were traveling and they didn't notice until later. The emotional signal was real. The actual behavior revealed the problem was not as severe as stated. Stated importance ≠ behavioral importance.

**The finance team feature request.** Team asks for internal messaging integrated into your tool. You start scoping a messaging feature. Then you dig: "Why do you want that?" — "So we can share files without emailing." The real problem was file sharing and version control. They needed Dropbox, not a chat feature. The feature request was a solution to an undiagnosed problem. Digging saved months of mis-directed development.

---

## The Ratio Rule

Track talk time. If you are talking more than the customer, the conversation is broken.

Rule: **The more you're talking, the worse you're doing.**

Your job is to ask one good question, then be quiet long enough for a complete answer. Resist the urge to elaborate on your question, fill silences, or follow up immediately. Silence is productive — it often produces the most honest and specific responses.

If you find yourself explaining, defending, or pitching: stop. You are in the wrong mode.

---

## Quick Diagnostic: Was That Meeting Useful?

After a conversation, apply this filter before logging it as signal.

Ask: Can I write down one specific fact about what this person did, paid, or changed in their behavior — with enough detail that it would survive scrutiny from a skeptic?

If yes: you have signal.

If your notes contain only "they loved it," "they were really interested," "we got great feedback," or "they said they'd buy it": you have noise.

Discard the noise. Return to the questions above. Redesign the next conversation to extract facts, not feelings.
